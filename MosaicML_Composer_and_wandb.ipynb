{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73de53e5-9e46-4edd-a64c-9778c3f7285e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/mosaicml/MosaicML_Composer_and_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--- @wandbcode{mosaicml} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a2eb0-65b5-4b6c-971a-19bf8121ab13",
   "metadata": {},
   "source": [
    "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<img src=\"https://raw.githubusercontent.com/mosaicml/composer/dev/docs/source/_static/images/header_dark.svg\" width=\"400\" alt=\"mosaicml\" />\n",
    "\n",
    "<!--- @wandbcode{lit_colab_boris} -->\n",
    "\n",
    "# Running fast with MosaicML Composer and Weight and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961c393-0154-4937-98d7-16e472b9a0d3",
   "metadata": {},
   "source": [
    "[MosaicML Composer](https://docs.mosaicml.com) is a library for training neural networks better, faster, and cheaper. It contains many state-of-the-art methods for accelerating neural network training and improving generalization, along with an optional Trainer API that makes composing many different enhancements easy.\n",
    "\n",
    "Coupled with [Weights & Biases integration](https://docs.mosaicml.com/en/v0.5.0/trainer/logging.html), you can quickly train and monitor models for full traceability and reproducibility with only 2 extra lines of code:\n",
    "\n",
    "```python\n",
    "from composer.loggers import WandBLogger\n",
    "wandb_logger = WandBLogger()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec877f6-57aa-4423-ae4b-f85769c59dd6",
   "metadata": {},
   "source": [
    "W&B integration with Composer can automatically:\n",
    "* log your configuration parameters\n",
    "* log your losses and metrics\n",
    "* log gradients and parameter distributions\n",
    "* log your model\n",
    "* keep track of your code\n",
    "* log your system metrics (GPU, CPU, memory, temperature, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b0104-530a-438d-bd68-08f627cc8920",
   "metadata": {
    "tags": []
   },
   "source": [
    "### üõ†Ô∏è Installation and set-up\n",
    "\n",
    "We need to install the following libraries:\n",
    "* [mosaicml-composer](https://docs.mosaicml.com/en/v0.5.0/getting_started/installation.html) to set up and train our models\n",
    "* [wandb](https://docs.wandb.ai/) to instrument our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bbcb21-babd-488b-a20d-080f43f09897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb mosaicml fastcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e54ec9-51b6-4f49-9ea1-2ed82f03add3",
   "metadata": {},
   "source": [
    "## üî• Getting Started with Composer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0f570-d323-4ecf-bdb3-1469730f562b",
   "metadata": {},
   "source": [
    "Composer gives you access to a set of functions to speedup your models and infuse them with state of the art methods. For instance, you can insert [BlurPool](https://docs.mosaicml.com/en/latest/method_cards/blurpool.html) into your CNN by calling `CF.apply_blurpool(model)` into your PyTorch model. Take a look at all the [functional](https://docs.mosaicml.com/en/latest/functional_api.html) methods available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d71a76-7354-43ee-87c1-a83aa88b0b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.utils.module_surgery:optimizers was not provided. Be sure to either create the optimizer after\n",
      "invoking this method, or manually add new parameters to the existing optimizer.\n",
      "INFO:composer.algorithms.blurpool.blurpool:Applied BlurPool to model ResNet. Model now has 1 BlurMaxPool2d and 6 BlurConv2D layers.\n",
      "INFO:composer.utils.module_surgery:optimizers was not provided. Be sure to either create the optimizer after\n",
      "invoking this method, or manually add new parameters to the existing optimizer.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from fastcore.all import *\n",
    "from composer import functional as CF\n",
    "import torchvision.models as models\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "model = models.resnet50()\n",
    "\n",
    "# replace some layers with blurpool\n",
    "CF.apply_blurpool(model);\n",
    "# replace some layers with squeeze-excite\n",
    "CF.apply_squeeze_excite(model, latent_channels=64, min_channels=128);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642eb0d5-b4f6-4add-9d59-235222bc2236",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using the `Trainer` class with Weights and Biases üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "\n",
    "W&B integration with MosaicML-Composer is built into the `Trainer` and can be configured to add extra functionalities through `WandBLogger`:\n",
    "\n",
    "* logging of Artifacts: Use `log_artifacts=True` to log model checkpoints as `wandb.Artifacts`. You can setup how often by passing an int value to `log_artifacts_every_n_batches` (default = 100)\n",
    "* you can also pass any parameter that you would pass to `wandb.init` in `init_params` as a dictionary. For example, you could pass `init_params = {\"project\":\"try_mosaicml\", \"name\":\"benchmark\", \"entity\":\"user_name\"}`.\n",
    "\n",
    "For more details refer to [Logger documentation](https://docs.mosaicml.com/en/latest/api_reference/composer.loggers.wandb_logger.html#composer.loggers.wandb_logger.WandBLogger) and [Wandb docs](https://docs.wandb.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98dee3-4a8f-4ac4-b92a-46ea279c9be2",
   "metadata": {},
   "source": [
    "Let's grab fastai's [Imagenette dataset](https://github.com/fastai/imagenette) and decompress it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b15541-9f20-4979-a24f-145546881641",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path('imagenette2.tgz').exists():\n",
    "    URL = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz'\n",
    "    !wget {URL}\n",
    "    imagenette_path = untar_dir(\"imagenette2.tgz\", Path(\".\"))\n",
    "else:\n",
    "    imagenette_path = Path('imagenette2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a222b9eb-4a30-4356-bcd7-38a6f08f1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.io as io\n",
    "\n",
    "class Imagenette:\n",
    "    def __init__(self, path, train=True, transform=None):\n",
    "        self.path = Path(path) / (\"train\" if train else \"val\")\n",
    "        self.transform = transform\n",
    "        self.files = list(path.glob(\"**/*.JPEG\"))\n",
    "        self.classes = {label.name:i for i, label in enumerate(self.path.iterdir())}\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        image = self.transform(io.read_image(str(file_path), \n",
    "                                             mode=io.image.ImageReadMode.RGB)) \n",
    "        label = self.classes[file_path.parent.name]\n",
    "        return image, label\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93fbba13-dbb9-436e-8237-fa60f0e54675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from composer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5424e4bf-bd7c-43ff-8a04-2d4e5e13afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_stats = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose([transforms.CenterCrop(256), \n",
    "                                transforms.ConvertImageDtype(torch.float),\n",
    "                                transforms.Normalize(*imagenet_stats)\n",
    "                               ])\n",
    "train_dataset = Imagenette(imagenette_path, train=True, transform=transform)\n",
    "eval_dataset = Imagenette(imagenette_path, train=False, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32, num_workers=8)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=64, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd120bee-0964-4e8f-8341-491b328ae974",
   "metadata": {},
   "source": [
    "A simple model and the `DecoupledAdamW` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5656bdcf-399f-408b-8368-a546d0fe6572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "from composer.models import ComposerModel\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class ResNet18(ComposerModel):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = resnet18(num_classes=num_classes)\n",
    "        self.train_accuracy = torchmetrics.Accuracy()\n",
    "        self.val_accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, batch): # batch is the output of the dataloader\n",
    "        # specify how batches are passed through the model\n",
    "        inputs, _ = batch\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def loss(self, outputs, batch):\n",
    "        # pass batches and `forward` outputs to the loss\n",
    "        _, targets = batch\n",
    "        return F.cross_entropy(outputs, targets)\n",
    "    \n",
    "    def validate(self, batch):\n",
    "\n",
    "        inputs, targets = batch\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs, targets\n",
    "    \n",
    "    def metrics(self, train=False):\n",
    "        # defines which metrics to use in each phase of training\n",
    "        return self.train_accuracy if train else self.val_accuracy\n",
    "    \n",
    "model = ResNet18(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa820b1-12bb-43c8-ae7c-5cea24b69c35",
   "metadata": {},
   "source": [
    "we define the `wandb.init` params here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd3d846b-e438-461f-9346-5818f64a180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.loggers import WandBLogger, TQDMLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4424591-cd28-4af2-987b-be0588a99a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = {\"project\":\"composer\", \n",
    "               \"name\":\"imagenette_baseline\"}\n",
    "\n",
    "# we pass the to the logger \n",
    "wandb_logger = WandBLogger(init_params=init_params)\n",
    "\n",
    "# we also add progressbar logging\n",
    "progress_logger = TQDMLogger()\n",
    "\n",
    "loggers = [progress_logger, wandb_logger]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b5740-7736-4f6d-88f1-c89a669691fe",
   "metadata": {},
   "source": [
    "to tweak what are we logging, we can pass `Callbacks` to the `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffda5f2a-3696-4da0-9197-17076d6dce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.callbacks import SpeedMonitor, LRMonitor, CheckpointSaver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e647bcc-abe5-45cd-bb7d-fa91e6c2006e",
   "metadata": {},
   "source": [
    "we include callbacks that measure the model throughput (and the learning rate) and logs them to Weights & Biases. `Callbacks` control what is being logged, whereas loggers specify where the information is being saved. For more information on loggers, see [Logging](https://docs.mosaicml.com/en/latest/trainer/logging.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e60c66bc-f046-4a57-9ea7-a6e9fe5cfc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [LRMonitor(),    # Logs the learning rate\n",
    "             SpeedMonitor(), # Logs the training throughput\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1cef41-583b-4915-88f0-a85bd4531206",
   "metadata": {},
   "source": [
    "we can also create a custom callback to log samples to `Weights and Biases` workspace,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4c0116c-70e0-4f34-828a-1c911c709346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from composer import Callback, State, Logger\n",
    "\n",
    "class LogPredictions(Callback):\n",
    "    def __init__(self, num_samples=100, seed=1234):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.data = []\n",
    "        \n",
    "    def eval_batch_end(self, state: State, logger: Logger):\n",
    "        \"\"\"Compute predictions per batch and stores them on self.data\"\"\"\n",
    "        \n",
    "        if state.timer.epoch == state.max_duration: #on last val epoch\n",
    "            if len(self.data) < self.num_samples:\n",
    "                n = self.num_samples\n",
    "                x, y = state.batch_pair\n",
    "                outputs = state.outputs.argmax(-1)\n",
    "                data = [[wandb.Image(x_i), y_i, y_pred] for x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))]\n",
    "                self.data += data\n",
    "            \n",
    "    def eval_end(self, state: State, logger: Logger):\n",
    "        \"Create a wandb.Table and logs it\"\n",
    "        columns = ['image', 'ground truth', 'prediction']\n",
    "        table = wandb.Table(columns=columns, data=self.data[:self.num_samples])\n",
    "        wandb.log({'sample_table':table}, step=int(state.timer.batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "682b52b0-5055-4cfd-a6c1-1a114390e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks.append(LogPredictions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541c9d6-fc72-46f4-941b-0c4dae64632b",
   "metadata": {},
   "source": [
    "then we pass them to the `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1721f138-3c72-4234-bf14-45235f1d776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.trainer.trainer:Setting seed to 3776208964\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/wandb/composer/wandb/run-20220322_172206-3h238z2p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/capecape/composer/runs/3h238z2p\" target=\"_blank\">imagenette_baseline</a></strong> to <a href=\"https://wandb.ai/capecape/composer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7e76286c6842c3927e365643857944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b833446d6b14d779c52a6831c244049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1, Batch 419 (val):   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdc37ef5ebb4138a9bfd941d7541157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d2f9eb8070439c9d99f78dafc61eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2, Batch 838 (val):   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3659a7b803048bbae33f5c55d77e3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f0f4204f774a9685e004f05dbc7204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3, Batch 1257 (val):   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8deae5acfc7c404aaff35bf070e6a310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebfd58cd3334d60a90125f92713f8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4, Batch 1676 (val):   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c08ef83be84a37b591731e6d1c30ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e86f82c24048e1aa61038f41040e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5, Batch 2095 (val):   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='9.111 MB of 9.111 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy/val</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà</td></tr><tr><td>loss/train</td><td>‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ</td></tr><tr><td>lr-Adam/group0</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>throughput/epoch</td><td>‚ñÅ‚ñà‚ñá‚ñá‚ñÜ</td></tr><tr><td>throughput/step</td><td>‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>trainer/batch_idx</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà</td></tr><tr><td>trainer/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>wall_clock_train</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy/val</td><td>0.66463</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>loss/train</td><td>0.91895</td></tr><tr><td>lr-Adam/group0</td><td>0.01</td></tr><tr><td>throughput/epoch</td><td>594.71717</td></tr><tr><td>throughput/step</td><td>610.55774</td></tr><tr><td>trainer/batch_idx</td><td>418</td></tr><tr><td>trainer/global_step</td><td>2095</td></tr><tr><td>wall_clock_train</td><td>112.92579</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">imagenette_baseline</strong>: <a href=\"https://wandb.ai/capecape/composer/runs/3h238z2p\" target=\"_blank\">https://wandb.ai/capecape/composer/runs/3h238z2p</a><br/>Synced 5 W&B file(s), 5 media file(s), 102 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220322_172206-3h238z2p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    optimizers=torch.optim.Adam(model.parameters(), lr=0.01),\n",
    "    max_duration=\"5ep\",\n",
    "    loggers=loggers,\n",
    "    callbacks=callbacks,\n",
    "    device=\"gpu\",\n",
    "\n",
    ")\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a36c37-e390-4e86-b2f0-2ec4c390aa80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Going üöÄ with the Trainer\n",
    "We can try some of the magic algorithms from Mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee9a72a3-07cb-4785-a989-be54501a4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.algorithms import LabelSmoothing, MixUp, ChannelsLast, ColOut, BlurPool\n",
    "from composer.optim import DecoupledAdamW, CosineAnnealingWithWarmupScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b470b0a-7534-43d2-95bb-fe6850352b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c617ebd7-5c82-49f2-8c8e-78fc42986198",
   "metadata": {},
   "outputs": [],
   "source": [
    "dadam = DecoupledAdamW(model.parameters(), lr=5e-2)\n",
    "\n",
    "cannel = CosineAnnealingWithWarmupScheduler('1ep', '1dur', alpha_f=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99cdec7f-8e8d-4e4d-a16e-7062b9234695",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = {\"project\":\"composer\", \n",
    "               \"name\":\"imagenette_algos\"}\n",
    "\n",
    "# we pass the to the logger \n",
    "wandb_logger = WandBLogger(init_params=init_params)\n",
    "\n",
    "# we also add progressbar logging\n",
    "progress_logger = TQDMLogger()\n",
    "\n",
    "loggers = [progress_logger, wandb_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "066c0b81-8da1-42bf-9682-dfbef156d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [LRMonitor(),    # Logs the learning rate\n",
    "             SpeedMonitor(), # Logs the training throughput\n",
    "             LogPredictions()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a67bddd-1bc5-4e5c-8e5c-a4f1690f5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms=[LabelSmoothing(), \n",
    "            MixUp(num_classes=10), \n",
    "            BlurPool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da9600-db4b-4a53-84d0-6bafe7daae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.trainer.trainer:Setting seed to 178492277\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/wandb/composer/wandb/run-20220322_173051-1jix7p3w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/capecape/composer/runs/1jix7p3w\" target=\"_blank\">imagenette_algos</a></strong> to <a href=\"https://wandb.ai/capecape/composer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.utils.module_surgery:adding 1 new parameters to parameter group #0\n",
      "INFO:composer.utils.module_surgery:adding 1 new parameters to parameter group #0\n",
      "INFO:composer.utils.module_surgery:adding 1 new parameters to parameter group #0\n",
      "INFO:composer.utils.module_surgery:adding 1 new parameters to parameter group #0\n",
      "INFO:composer.utils.module_surgery:adding 1 new parameters to parameter group #0\n",
      "INFO:composer.utils.module_surgery:adding 1 new parameters to parameter group #0\n",
      "INFO:composer.algorithms.blurpool.blurpool:Applied BlurPool to model ResNet18. Model now has 1 BlurMaxPool2d and 12 BlurConv2D layers.\n",
      "INFO:composer.algorithms.blurpool.blurpool:Applied BlurPool to model ResNet18 with replace_maxpools=True, replace_convs=True. Model now has 1 BlurMaxPool2d and 12 BlurConv2D layers.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    max_duration=\"5ep\",\n",
    "    loggers=loggers,\n",
    "    callbacks=callbacks,\n",
    "    optimizers=dadam,\n",
    "    schedulers=cannel,\n",
    "    algorithms=algorithms,\n",
    "    precision=\"amp\",\n",
    "    device=\"gpu\",\n",
    ")\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2572e-6427-4338-954c-466526c3859f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
